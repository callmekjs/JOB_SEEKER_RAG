import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
import json
import re
from datetime import datetime

try:
    from playwright.sync_api import sync_playwright
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False

class JumpitCrawler:
    """ì í•(jumpit.saramin.co.kr) ì±„ìš© ê³µê³  í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://jumpit.saramin.co.kr/',
        }
        self.jumpit_base_url = "https://jumpit.saramin.co.kr"
        self.jumpit_positions_url = f"{self.jumpit_base_url}/positions"

        # ê°œë°œ ì§ë¬´ íƒìƒ‰ (ì í• ì‚¬ì´íŠ¸ ì§ë¬´ í•„í„° ê°’)
        self.jumpit_job_roles = {
            'ì „ì²´': None,
            'ì„œë²„/ë°±ì—”ë“œ ê°œë°œì': 'backend',
            'í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì': 'frontend',
            'ì›¹ í’€ìŠ¤íƒ ê°œë°œì': 'fullstack',
            'ì•ˆë“œë¡œì´ë“œ ê°œë°œì': 'android',
            'iOS ê°œë°œì': 'ios',
            'í¬ë¡œìŠ¤í”Œë«í¼ ì•±ê°œë°œì': 'crossplatform',
            'ê²Œì„ í´ë¼ì´ì–¸íŠ¸ ê°œë°œì': 'game-client',
            'ê²Œì„ ì„œë²„ ê°œë°œì': 'game-server',
            'DBA': 'dba',
            'ë¹…ë°ì´í„° ì—”ì§€ë‹ˆì–´': 'bigdata',
            'ì¸ê³µì§€ëŠ¥/ë¨¸ì‹ ëŸ¬ë‹': 'ai-ml',
            'devops/ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´': 'devops',
            'ì •ë³´ë³´ì•ˆ ë‹´ë‹¹ì': 'security',
            'QA ì—”ì§€ë‹ˆì–´': 'qa',
            'ê°œë°œ PM': 'pm',
            'HW/ì„ë² ë””ë“œ': 'embedded',
            'SW/ì†”ë£¨ì…˜': 'solution',
            'ì›¹í¼ë¸”ë¦¬ì…”': 'webpub',
            'VR/AR/3D': 'vr-ar',
            'ë¸”ë¡ì²´ì¸': 'blockchain',
            'ê¸°ìˆ ì§€ì›': 'tech-support',
        }

    def search_jobs_jumpit(self, sort='popular', max_pages=30, job_role='ì „ì²´', target_count=None):
        """ì í• positions í˜ì´ì§€ì—ì„œ ì±„ìš© ê³µê³  í¬ë¡¤ë§. target_count ì§€ì • ì‹œ í•´ë‹¹ ê°œìˆ˜ ì±„ìš¸ ë•Œê¹Œì§€ ì—°ì† í˜ì´ì§€ ìˆ˜ì§‘."""
        jobs = []
        url = self.jumpit_positions_url
        params = {'sort': sort}
        role_param = self.jumpit_job_roles.get(job_role) if job_role else None
        if role_param:
            params['job'] = role_param
        use_playwright = False

        try:
            page = 1
            seen_links = set()
            while page <= max_pages:
                if page > 1:
                    params['page'] = page
                role_label = f", ì§ë¬´={job_role}" if job_role else ""
                unique_so_far = len(seen_links)
                print(f"ğŸ“„ ì í• positions {page} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... (sort={sort}{role_label}) [ê³ ìœ  {unique_so_far}ê°œ]")

                html = None
                if use_playwright and HAS_PLAYWRIGHT:
                    html = self._fetch_html_with_playwright(url, params)
                else:
                    response = requests.get(url, params=params, headers=self.headers)
                    response.raise_for_status()
                    html = response.text

                next_data = self._parse_next_data(html or '')
                page_jobs = []
                if next_data:
                    page_jobs = self._extract_jobs_from_jumpit_data(next_data, job_role=job_role or 'ì „ì²´')
                if not page_jobs and page == 1:
                    next_data = self._fetch_next_data_url(params)
                    if next_data:
                        page_jobs = self._extract_jobs_from_jumpit_data(next_data, job_role=job_role or 'ì „ì²´')
                if not page_jobs and page == 1 and HAS_PLAYWRIGHT:
                    print("   â””â”€ ë¸Œë¼ìš°ì €(Playwright)ë¡œ í˜ì´ì§€ ë¡œë“œ ì¤‘...")
                    html = self._fetch_html_with_playwright(url, params)
                    if html:
                        next_data = self._parse_next_data(html)
                        if next_data:
                            page_jobs = self._extract_jobs_from_jumpit_data(next_data, job_role=job_role or 'ì „ì²´')
                        if not page_jobs:
                            page_jobs = self._extract_jobs_from_jumpit_html(html, job_role=job_role or 'ì „ì²´')
                        if page_jobs:
                            use_playwright = True
                if not page_jobs:
                    page_jobs = self._extract_jobs_from_jumpit_html(html or '', job_role=job_role or 'ì „ì²´')

                if page_jobs:
                    jobs.extend(page_jobs)
                    for j in page_jobs:
                        link = j.get('link')
                        if link:
                            seen_links.add(link)
                    unique_so_far = len(seen_links)
                    print(f"   â””â”€ {len(page_jobs)}ê°œ ìˆ˜ì§‘ (ê³ ìœ  {unique_so_far}ê°œ)")
                    if target_count and unique_so_far >= target_count:
                        print(f"   â†’ ëª©í‘œ ê³ ìœ  {target_count}ê°œ ë„ë‹¬, ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¢…ë£Œ")
                        break
                else:
                    if page == 1:
                        if not HAS_PLAYWRIGHT:
                            print("   âš ï¸ ë°ì´í„° ì—†ìŒ. pip install playwright í›„ playwright install chromium ì‹¤í–‰í•˜ì„¸ìš”.")
                        else:
                            print("   âš ï¸ ë¸Œë¼ìš°ì € ë¡œë“œ í›„ì—ë„ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    break

                page += 1
                time.sleep(1)

            print(f"âœ… ì í• '{job_role or "ì „ì²´"}' (sort={sort}) ì´ {len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì í• í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []

        return jobs

    def _parse_next_data(self, html):
        """HTMLì—ì„œ __NEXT_DATA__ ë˜ëŠ” ìœ ì‚¬ JSON ë¸”ë¡ ì¶”ì¶œ"""
        soup = BeautifulSoup(html, 'html.parser')
        script = soup.find('script', id='__NEXT_DATA__')
        if script and script.string:
            try:
                return json.loads(script.string)
            except json.JSONDecodeError:
                pass
        for s in soup.find_all('script', type='application/json'):
            if s.string and ('position' in s.string.lower() or 'recruit' in s.string.lower()):
                try:
                    return json.loads(s.string)
                except json.JSONDecodeError:
                    continue
        return None

    def _fetch_next_data_url(self, params):
        """Next.js _next/data APIë¡œ í¬ì§€ì…˜ ë°ì´í„° ì§ì ‘ ìš”ì²­ (buildId ì¶”ì¶œ í›„)"""
        try:
            r = requests.get(
                self.jumpit_positions_url,
                params={k: v for k, v in params.items() if k != 'page'},
                headers=self.headers,
            )
            r.raise_for_status()
            html = r.text
            data = self._parse_next_data(html)
            build_id = None
            if data and data.get('buildId'):
                build_id = data['buildId']
            if not build_id:
                build_match = re.search(r'/_next/data/([a-zA-Z0-9_-]+)/', html)
                if build_match:
                    build_id = build_match.group(1)
            if build_id:
                data_url = f"{self.jumpit_base_url}/_next/data/{build_id}/positions.json"
                resp = requests.get(data_url, params=params, headers={**self.headers, 'Accept': 'application/json'})
                resp.raise_for_status()
                return resp.json()
        except Exception as e:
            print(f"   âš ï¸ _next/data ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None

    def _fetch_html_with_playwright(self, base_url, params, timeout=20000):
        """Playwrightë¡œ í˜ì´ì§€ë¥¼ ë Œë”ë§í•œ ë’¤ HTML ë°˜í™˜ (JS ë¡œë“œ ëŒ€ê¸°)"""
        if not HAS_PLAYWRIGHT:
            return None
        from urllib.parse import urlencode
        qs = urlencode(params)
        url = f"{base_url}?{qs}" if qs else base_url
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                page.goto(url, wait_until="domcontentloaded", timeout=timeout)
                page.wait_for_selector('a[href*="/position/"]', timeout=timeout)
                html = page.content()
                browser.close()
            return html
        except Exception as e:
            print(f"   âš ï¸ Playwright ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _fetch_jobs_list_infinite_scroll_playwright(self, target_count=200, job_role='ì „ì²´', sort='popular'):
        """ë¬´í•œ ìŠ¤í¬ë¡¤ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ target_countê°œ ì¹´ë“œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ìŠ¤í¬ë¡¤ í›„ ì¹´ë“œ ëª©ë¡ ìˆ˜ì§‘."""
        if not HAS_PLAYWRIGHT:
            return []
        from urllib.parse import urlencode
        params = {'sort': sort}
        role_param = self.jumpit_job_roles.get(job_role) if job_role else None
        if role_param:
            params['job'] = role_param
        list_url = self.jumpit_positions_url + ('?' + urlencode(params) if params else '')
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                page.goto(list_url, wait_until="domcontentloaded", timeout=20000)
                page.wait_for_selector('a[href*="/position/"]', timeout=15000)
                print(f"\nğŸ“œ ë¬´í•œ ìŠ¤í¬ë¡¤: ìµœì†Œ {target_count}ê°œ ì¹´ë“œ ë¡œë“œë  ë•Œê¹Œì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
                max_scrolls = 40
                scroll_pause_sec = 1.5
                for scroll_num in range(max_scrolls):
                    prev_height = page.evaluate("document.body.scrollHeight")
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(scroll_pause_sec)
                    links = page.evaluate("""() => {
                        const as = Array.from(document.querySelectorAll('a[href*="/position/"]'));
                        return as.map(a => a.href).filter(h => h && h.includes('/position/'));
                    }""")
                    seen = set()
                    unique_links = [u for u in links if u not in seen and not seen.add(u)]
                    count = len(unique_links)
                    print(f"   ìŠ¤í¬ë¡¤ {scroll_num + 1}/{max_scrolls} â€” ë¡œë“œëœ ê³ ìœ  ì¹´ë“œ: {count}ê°œ")
                    if count >= target_count:
                        print(f"   â†’ ëª©í‘œ {target_count}ê°œ ë„ë‹¬")
                        break
                    new_height = page.evaluate("document.body.scrollHeight")
                    if new_height == prev_height and count > 0:
                        time.sleep(0.5)
                        new_height = page.evaluate("document.body.scrollHeight")
                    if new_height == prev_height:
                        break
                html = page.content()
                browser.close()
            jobs = self._extract_jobs_from_jumpit_html(html, job_role=job_role or 'ì „ì²´')
            seen_links = set()
            unique_jobs = []
            for j in jobs:
                link = j.get('link')
                if link and link not in seen_links:
                    seen_links.add(link)
                    unique_jobs.append(j)
                    if len(unique_jobs) >= target_count:
                        break
            print(f"âœ… ë¬´í•œ ìŠ¤í¬ë¡¤ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: ê³ ìœ  {len(unique_jobs)}ê°œ")
            return unique_jobs
        except Exception as e:
            print(f"   âš ï¸ ë¬´í•œ ìŠ¤í¬ë¡¤ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def _find_positions_list_in_json(self, obj, depth=0, max_depth=15):
        """JSON íŠ¸ë¦¬ë¥¼ ì¬ê·€ íƒìƒ‰í•´ í¬ì§€ì…˜ì²˜ëŸ¼ ë³´ì´ëŠ” ë¦¬ìŠ¤íŠ¸(ê°ì²´ì— id/title ë“±) ì°¾ê¸°"""
        if depth > max_depth:
            return None
        if isinstance(obj, list):
            if len(obj) == 0:
                return None
            first = obj[0]
            if isinstance(first, dict):
                has_id = 'id' in first or 'positionId' in first or 'recruitNo' in first
                has_title = 'title' in first or 'positionTitle' in first or 'jobTitle' in first
                has_company = 'company' in first or 'companyName' in first
                if has_id or (has_title and (has_company or 'link' in first or 'url' in first)):
                    return obj
            return None
        if isinstance(obj, dict):
            for v in obj.values():
                found = self._find_positions_list_in_json(v, depth + 1, max_depth)
                if found:
                    return found
        return None

    def _extract_jobs_from_jumpit_data(self, data, job_role='ì „ì²´'):
        """__NEXT_DATA__ ë“± JSONì—ì„œ í¬ì§€ì…˜ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
        jobs = []
        try:
            props = data.get('props', {}).get('pageProps') or data.get('pageProps') or data
            raw_list = (
                props.get('positions') or props.get('positionList') or
                props.get('jobs') or props.get('recruitList')
            )
            if not raw_list and isinstance(props.get('dehydratedState'), dict):
                qs = props.get('dehydratedState', {}).get('queries') or []
                for q in qs:
                    state = (q or {}).get('state', {}) or {}
                    data_inner = state.get('data') if isinstance(state, dict) else None
                    if isinstance(data_inner, dict):
                        raw_list = data_inner.get('positions') or data_inner.get('positionList') or data_inner.get('jobs')
                    if raw_list:
                        break
            if not raw_list:
                raw_list = self._find_positions_list_in_json(data)
            if not raw_list:
                raw_list = []
            for item in (raw_list if isinstance(raw_list, list) else []):
                job = self._normalize_jumpit_job(item, job_role=job_role)
                if job:
                    jobs.append(job)
        except Exception as e:
            print(f"   âš ï¸ JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return jobs

    def _normalize_jumpit_job(self, item, job_role='ì „ì²´'):
        """ì í• API/JSON í•­ëª©ì„ ê³µí†µ job ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        if not isinstance(item, dict):
            return None
        try:
            title = item.get('title') or item.get('positionTitle') or item.get('jobTitle') or ''
            company = item.get('company', {}).get('name', '') if isinstance(item.get('company'), dict) else item.get('companyName') or item.get('company') or ''
            if not company and isinstance(item.get('company'), str):
                company = item.get('company', '')
            pos_id = str(item.get('id') or item.get('positionId') or item.get('recruitNo') or item.get('positionNo') or '')
            if not title and not pos_id:
                return None
            link = item.get('url') or item.get('link') or f"{self.jumpit_base_url}/position/{pos_id}" if pos_id else ''
            if link and not link.startswith('http'):
                link = f"{self.jumpit_base_url}{link}" if link.startswith('/') else f"{self.jumpit_base_url}/position/{pos_id}"
            location = 'ì§€ì—­ ì—†ìŒ'
            if isinstance(item.get('location'), str):
                location = item.get('location')
            elif isinstance(item.get('locations'), list) and item['locations']:
                location = ', '.join(str(x) for x in item['locations'][:3])
            career = item.get('career') or item.get('careerLevel') or item.get('experience') or 'ê²½ë ¥ ì—†ìŒ'
            return {
                'job_role': title,
                'title': company,
                'company': company,
                'location': location,
                'career': career,
                'education': item.get('education') or 'í•™ë ¥ ì—†ìŒ',
                'work_type': item.get('employmentType') or item.get('workType') or 'ê·¼ë¬´í˜•íƒœ ì—†ìŒ',
                'deadline': item.get('deadline') or item.get('endDate') or '',
                'link': link,
                'rec_idx': pos_id,
                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'company_years': '',
                'tech_stack': '',
                'main_tasks': '',
                'qualifications': '',
                'preferred': '',
                'benefits': '',
                'recruitment_process': '',
            }
        except Exception:
            return None

    def _parse_card_text(self, full_text):
        """ì¹´ë“œ í•œ ë©ì–´ë¦¬ í…ìŠ¤íŠ¸ë¥¼ title, company, tech_stack, location, career ë“±ìœ¼ë¡œ ë¶„ë¦¬"""
        t = (full_text or '').strip()
        out = {'deadline': '', 'company': '', 'title': '', 'tech_stack': '', 'location': 'ì§€ì—­ ì—†ìŒ', 'career': 'ê²½ë ¥ ì—†ìŒ'}
        if not t or len(t) < 4:
            return out
        # D-8 ê°™ì€ ë§ˆê° ë°°ì§€
        badge = re.match(r'^(D-\d+)', t)
        if badge:
            out['deadline'] = badge.group(1)
            t = t[badge.end():].strip()
        # ëì—ì„œë¶€í„°: ê²½ë ¥ â†’ ì§€ì—­ â†’ ê¸°ìˆ ìŠ¤íƒ ìˆœìœ¼ë¡œ ì œê±°
        career_m = re.search(r'(ì‹ ì…|ê²½ë ¥\s*[\d~ë…„]+)\s*$', t)
        if career_m:
            out['career'] = career_m.group(1).strip()
            t = t[:career_m.start()].strip()
        loc_m = re.search(r'((?:ì„œìš¸|ê²½ê¸°|ì¸ì²œ|ë¶€ì‚°|ëŒ€êµ¬|ëŒ€ì „|ê´‘ì£¼|ì„¸ì¢…|ì œì£¼)[ê°€-í£0-9\s,Â·]*)\s*$', t)
        if loc_m:
            out['location'] = loc_m.group(1).strip()
            t = t[:loc_m.start()].strip()
        tech_m = re.search(r'([\w/Â·\s]+(?:Â·\s*[\w/]+\s*)+)', t)
        if tech_m:
            out['tech_stack'] = tech_m.group(1).strip().replace('Â·', ',').replace(' ,', ',').strip()
            t = (t[:tech_m.start()] + t[tech_m.end():]).strip()
        # ë‚¨ì€ ë¶€ë¶„: íšŒì‚¬ëª…(í•œê¸€) + ì§ë¬´ì œëª© (ì˜ˆ: ì—ìŠ¤í”¼ì—ì´ì¹˜B2B í”„ë¡œì íŠ¸ ê°œë°œíŒ€ ì‹ ì…)
        remainder = t.strip()
        if remainder:
            company_m = re.match(r'^([ê°€-í£]+)', remainder)
            if company_m:
                out['company'] = company_m.group(1)
                rest = remainder[company_m.end():].strip()
                out['title'] = rest if rest else remainder
            else:
                out['title'] = remainder
        return out

    def _parse_list_card_selectors(self, card):
        """ì¹´ë“œ(BeautifulSoup ìš”ì†Œ) ë‚´ë¶€ì—ì„œ ë°ë“œë¼ì¸Â·job_roleÂ·íšŒì‚¬ëª…ë§Œ ì…€ë ‰í„°ë¡œ ì¶”ì¶œ.
        - ë°ë“œë¼ì¸: span.czeWCl (ë˜ëŠ” class*='czeWCl')
        - job_role: h2.position_card_info_title
        - íšŒì‚¬ëª…: ë°ë“œë¼ì¸ spanì´ ì•„ë‹Œ span (íšŒì‚¬ì´ë¦„)
        """
        out = {'deadline': '', 'job_role': '', 'company': ''}
        if not card:
            return out
        deadline_el = card.select_one('span.czeWCl') or card.select_one('span[class*="czeWCl"]') or card.select_one('span[class*="sc-a0b0873a-0"]')
        if deadline_el:
            out['deadline'] = (deadline_el.get_text(strip=True) or '').strip()
        title_el = card.select_one('h2.position_card_info_title')
        if title_el:
            out['job_role'] = (title_el.get_text(strip=True) or '').strip()
        for span in card.select('span'):
            classes = span.get('class') or []
            if 'czeWCl' in classes:
                continue
            if 'sc-a0b0873a-0' in classes:
                continue
            t = (span.get_text(strip=True) or '').strip()
            if t and len(t) < 100:
                out['company'] = t
                break
        return out

    def _extract_jobs_from_jumpit_html(self, html, job_role='ì „ì²´'):
        """ì í• HTMLì—ì„œ í¬ì§€ì…˜ ë§í¬/ì¹´ë“œë¡œ ê³µê³  ì¶”ì¶œ. ì¹´ë“œ ë‚´ ë°ë“œë¼ì¸Â·job_roleÂ·íšŒì‚¬ëª…ì€ ì…€ë ‰í„°ë¡œ ì¶”ì¶œ."""
        jobs = []
        soup = BeautifulSoup(html, 'html.parser')
        for a in soup.select('a[href*="/position/"]'):
            href = a.get('href', '')
            match = re.search(r'/position/(\d+)', href)
            if not match:
                continue
            pos_id = match.group(1)
            link = f"{self.jumpit_base_url}/position/{pos_id}" if not href.startswith('http') else href.split('?')[0]
            raw_text = a.get_text(strip=True) or ''
            if len(raw_text) < 2 or len(raw_text) > 500:
                continue
            parsed = self._parse_list_card_selectors(a)
            fallback = self._parse_card_text(raw_text)
            if not parsed['job_role'] and not parsed['company']:
                parsed['deadline'] = parsed['deadline'] or fallback['deadline']
                parsed['job_role'] = fallback['title']
                parsed['company'] = fallback['company']
            job = {
                'job_role': parsed['job_role'] or '',
                'title': parsed['company'] or '',
                'company': parsed['company'] or '',
                'location': fallback.get('location') or 'ì§€ì—­ ì—†ìŒ',
                'career': fallback.get('career') or 'ê²½ë ¥ ì—†ìŒ',
                'education': 'í•™ë ¥ ì—†ìŒ',
                'work_type': 'ê·¼ë¬´í˜•íƒœ ì—†ìŒ',
                'deadline': parsed['deadline'] or '',
                'link': link,
                'rec_idx': pos_id,
                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'company_years': '',
                'tech_stack': fallback.get('tech_stack') or '',
                'main_tasks': '',
                'qualifications': '',
                'preferred': '',
                'benefits': '',
                'recruitment_process': '',
            }
            jobs.append(job)
        # ì¤‘ë³µ ì œê±° (ê°™ì€ rec_idx)
        seen = set()
        unique = []
        for j in jobs:
            if j['rec_idx'] not in seen:
                seen.add(j['rec_idx'])
                unique.append(j)
        return unique

    def _parse_position_detail_from_next_data(self, data):
        """__NEXT_DATA__ì—ì„œ ìƒì„¸ í•„ë“œ ì¶”ì¶œ (ê¸°ìˆ ìŠ¤íƒ, ì£¼ìš”ì—…ë¬´, ìê²©ìš”ê±´ ë“±)"""
        result = {
            'position_title': '',
            'company_name': '',
            'company_years': '',
            'tech_stack': '',
            'main_tasks': '',
            'qualifications': '',
            'preferred': '',
            'benefits': '',
            'recruitment_process': '',
            'career': '',
            'education': '',
            'deadline': '',
            'location': '',
        }
        try:
            props = data.get('props', {}).get('pageProps', data)
            pos = props.get('position') or props.get('positionDetail') or props.get('data')
            if not isinstance(pos, dict) and isinstance(props.get('dehydratedState'), dict):
                for q in (props.get('dehydratedState', {}).get('queries') or []):
                    state = (q or {}).get('state', {})
                    data_obj = state.get('data') or {}
                    if isinstance(data_obj, dict) and (data_obj.get('title') or data_obj.get('id')):
                        pos = data_obj
                        break
            if not isinstance(pos, dict):
                pos = props if isinstance(props, dict) else {}

            # ì§ë¬´ ì œëª©Â·íšŒì‚¬ëª… (job_role / title ë³´ì •ìš©)
            result['position_title'] = (pos.get('title') or pos.get('positionTitle') or pos.get('jobTitle') or '').strip()
            company_obj = pos.get('company') if isinstance(pos.get('company'), dict) else {}
            result['company_name'] = (company_obj.get('name', '') if company_obj else pos.get('companyName') or pos.get('company') or '')
            if isinstance(result['company_name'], str):
                result['company_name'] = result['company_name'].strip()
            else:
                result['company_name'] = ''

            def to_text(val):
                if val is None:
                    return ''
                if isinstance(val, str):
                    return val.strip()
                if isinstance(val, list):
                    return '\n'.join(to_text(x) for x in val).strip()
                if isinstance(val, dict) and val.get('name'):
                    return str(val.get('name', '')).strip()
                return str(val).strip()

            def list_to_text(items, sep='\n'):
                if not items:
                    return ''
                if isinstance(items, str):
                    return items
                return sep.join(to_text(x) for x in items if to_text(x))

            # ê¸°ìˆ ìŠ¤íƒ: techStack, skillTags, technologies, tech_stack ë“±
            tech = pos.get('techStack') or pos.get('skillTags') or pos.get('technologies') or pos.get('tech_stack') or []
            if isinstance(tech, str):
                result['tech_stack'] = tech
            else:
                result['tech_stack'] = list_to_text(tech, ', ') or list_to_text(tech)

            # ì£¼ìš”ì—…ë¬´: mainTasks, mainTasksList, responsibilities, main_tasks
            main = pos.get('mainTasks') or pos.get('mainTasksList') or pos.get('responsibilities') or pos.get('main_tasks')
            result['main_tasks'] = list_to_text(main) if main else ''

            # ìê²©ìš”ê±´: qualifications, requirements, qualification
            qual = pos.get('qualifications') or pos.get('requirements') or pos.get('qualification')
            result['qualifications'] = list_to_text(qual) if qual else ''

            # ìš°ëŒ€ì‚¬í•­: preferred, preferredQualifications, preferredRequirements
            pref = pos.get('preferred') or pos.get('preferredQualifications') or pos.get('preferredRequirements') or pos.get('ìš°ëŒ€ì‚¬í•­')
            result['preferred'] = list_to_text(pref) if pref else ''

            # ë³µì§€ ë° í˜œíƒ: benefits, welfare, perks
            ben = pos.get('benefits') or pos.get('welfare') or pos.get('perks') or pos.get('ë³µì§€')
            result['benefits'] = list_to_text(ben) if ben else ''

            # ì±„ìš©ì ˆì°¨ ë° ê¸°íƒ€: recruitmentProcess, process, applicationGuide
            proc = pos.get('recruitmentProcess') or pos.get('process') or pos.get('applicationGuide') or pos.get('ì±„ìš©ì ˆì°¨')
            result['recruitment_process'] = list_to_text(proc) if proc else to_text(proc)

            # ì—…ë ¥ (íšŒì‚¬ ì„¤ë¦½/ê²½ë ¥ ë…„ìˆ˜)
            company_obj = pos.get('company') if isinstance(pos.get('company'), dict) else {}
            result['company_years'] = to_text(
                company_obj.get('companyYears') or company_obj.get('yearsInBusiness') or
                company_obj.get('ì—…ë ¥') or pos.get('companyYears') or pos.get('ì—…ë ¥')
            )

            # ê²½ë ¥ / í•™ë ¥ / ë§ˆê°ì¼ / ê·¼ë¬´ì§€ì—­
            result['career'] = to_text(pos.get('career') or pos.get('careerLevel') or pos.get('experience') or result['career'])
            result['education'] = to_text(pos.get('education') or pos.get('educationLevel') or result['education'])
            result['deadline'] = to_text(pos.get('deadline') or pos.get('endDate') or pos.get('dueDate') or result['deadline'])
            result['location'] = to_text(pos.get('location') or pos.get('workLocation') or pos.get('address') or pos.get('workPlace') or result['location'])

            # ì£¼ì†Œê°€ ê°ì²´ì¸ ê²½ìš° (addressDetail ë“±)
            addr = pos.get('addressDetail') or pos.get('address')
            if isinstance(addr, dict):
                result['location'] = to_text(addr.get('fullAddress') or addr.get('address') or addr.get('name')) or result['location']
            elif addr and not result['location']:
                result['location'] = to_text(addr)

            return result
        except Exception as e:
            print(f"   âš ï¸ ìƒì„¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def _parse_position_detail_from_html(self, html):
        """HTMLì—ì„œ ì„¹ì…˜ë³„ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (__NEXT_DATA__ ì—†ì„ ë•Œ)"""
        result = {
            'company_years': '',
            'tech_stack': '',
            'main_tasks': '',
            'qualifications': '',
            'preferred': '',
            'benefits': '',
            'recruitment_process': '',
            'career': '',
            'education': '',
            'deadline': '',
            'location': '',
        }
        soup = BeautifulSoup(html, 'html.parser')
        # ì—…ë ¥: dl.details ë‚´ dt "ì—…ë ¥" ë‹¤ìŒ dd í…ìŠ¤íŠ¸ (ì˜ˆ: 18ë…„ì°¨(2009ë…„ 6ì›” ì„¤ë¦½))
        dt_up = soup.find('dt', string=re.compile(re.escape('ì—…ë ¥')))
        if dt_up:
            dd = dt_up.find_next_sibling('dd') or dt_up.find_next('dd')
            if dd:
                result['company_years'] = (dd.get_text(strip=True) or '').strip()
        # div.position_info ë‚´ dl > dt / dd: ê¸°ìˆ ìŠ¤íƒ, ì£¼ìš”ì—…ë¬´, ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­, ë³µì§€ ë° í˜œíƒ, ì±„ìš©ì ˆì°¨
        section_labels = {
            'ê¸°ìˆ ìŠ¤íƒ': 'tech_stack',
            'ì£¼ìš”ì—…ë¬´': 'main_tasks',
            'ìê²©ìš”ê±´': 'qualifications',
            'ìš°ëŒ€ì‚¬í•­': 'preferred',
            'ë³µì§€ ë° í˜œíƒ': 'benefits',
            'ì±„ìš©ì ˆì°¨ ë° ê¸°íƒ€ ì§€ì› ìœ ì˜ì‚¬í•­': 'recruitment_process',
        }
        position_info = soup.select_one('div.position_info')
        scope = position_info if position_info else soup
        for dt in scope.find_all('dt'):
            label_text = (dt.get_text(strip=True) or '').strip()
            if not label_text:
                continue
            for label, key in section_labels.items():
                if label == label_text or label in label_text:
                    dd = dt.find_next_sibling('dd') or dt.find_next('dd')
                    if dd:
                        raw = dd.get_text(separator='\n', strip=True) or ''
                        if key == 'tech_stack':
                            result[key] = raw.replace('\n', ', ').strip()
                        else:
                            result[key] = raw.strip()
                    break
        # ê²½ë ¥/í•™ë ¥/ë§ˆê°ì¼/ê·¼ë¬´ì§€ì—­: dl > dt/dd (í¬ì§€ì…˜ ê²½ë ¥Â·í•™ë ¥Â·ë§ˆê°ì¼Â·ê·¼ë¬´ì§€ì—­ ì •ë³´ ë¸”ë¡)
        meta_labels = {'ê²½ë ¥': 'career', 'í•™ë ¥': 'education', 'ë§ˆê°ì¼': 'deadline', 'ê·¼ë¬´ì§€ì—­': 'location'}
        for dt in soup.find_all('dt'):
            label_text = (dt.get_text(strip=True) or '').strip()
            if label_text not in meta_labels:
                continue
            dd = dt.find_next_sibling('dd') or dt.find_next('dd')
            if dd:
                result[meta_labels[label_text]] = (dd.get_text(separator=' ', strip=True) or '').strip()
        return result

    def _enrich_jobs_with_details_playwright(self, jobs, max_details=20, list_url=None):
        """Playwrightë¡œ ê° ì¹´ë“œ(ìƒì„¸ í˜ì´ì§€) ì ‘ì† â†’ ë¡œë”© ëŒ€ê¸° â†’ ìƒì„¸ ë°ì´í„° ì¶”ì¶œ. list_url ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸â†’ìƒì„¸â†’ì´ì „ í˜ì´ì§€ ìˆœíšŒ."""
        if not HAS_PLAYWRIGHT or not jobs:
            return jobs
        to_fetch = jobs[:max_details] if max_details else jobs
        total = len(to_fetch)
        print(f"\nğŸ“‹ ì¹´ë“œë³„ ìƒì„¸ ìˆ˜ì§‘ (ë¸Œë¼ìš°ì €ì—ì„œ ê° ì¹´ë“œ ì ‘ì†): ìµœëŒ€ {total}ê±´")
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                if list_url:
                    page.goto(list_url, wait_until="domcontentloaded", timeout=20000)
                    time.sleep(1)
                for i, job in enumerate(to_fetch):
                    link = job.get('link')
                    if not link:
                        continue
                    try:
                        print(f"   [{i + 1}/{total}] ì¹´ë“œ ì ‘ì† ì¤‘: {job.get('title', '')[:40]}...")
                        page.goto(link, wait_until="domcontentloaded", timeout=20000)
                        try:
                            page.wait_for_selector('script#__NEXT_DATA__', timeout=10000, state='attached')
                        except Exception:
                            pass  # __NEXT_DATA__ ì—†ìœ¼ë©´ HTML í´ë°±ìœ¼ë¡œ ì§„í–‰
                        html = page.content()
                        next_data = self._parse_next_data(html)
                        if next_data:
                            detail = self._parse_position_detail_from_next_data(next_data)
                        else:
                            detail = self._parse_position_detail_from_html(html)
                        if detail:
                            # ìƒì„¸ì—ì„œ ì§ë¬´Â·íšŒì‚¬ëª… ìˆìœ¼ë©´ job_role / title ë³´ì • (ë¦¬ìŠ¤íŠ¸ íŒŒì‹± ì˜¤ë¥˜ í•´ì†Œ)
                            pt = (detail.get('position_title') or '').strip()
                            if pt:
                                job['job_role'] = pt
                            cn = (detail.get('company_name') or '').strip()
                            if cn:
                                job['title'] = cn
                                job['company'] = cn
                            for k in ('company_years', 'tech_stack', 'main_tasks', 'qualifications', 'preferred', 'benefits', 'recruitment_process'):
                                job[k] = (detail.get(k) or '').strip()
                            for k in ('career', 'education', 'deadline', 'location'):
                                v = (detail.get(k) or '').strip()
                                if v:
                                    job[k] = v
                        if list_url:
                            page.go_back()
                            time.sleep(0.5)
                    except Exception as e:
                        print(f"   âš ï¸ [{i + 1}] ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    time.sleep(1)
                browser.close()
        except Exception as e:
            print(f"   âš ï¸ Playwright ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        print("âœ… ìƒì„¸ ìˆ˜ì§‘ ì™„ë£Œ!")
        return jobs

    def enrich_jobs_with_details(self, jobs, max_details=50, list_url=None):
        """ë¦¬ìŠ¤íŠ¸ ì¹´ë“œ â†’ ê° ì¹´ë“œ(ìƒì„¸ í˜ì´ì§€) Playwrightë¡œ ì ‘ì† í›„ ê¸°ìˆ ìŠ¤íƒÂ·ì£¼ìš”ì—…ë¬´ ë“± ìˆ˜ì§‘ (Playwright ì „ìš©). list_url ìˆìœ¼ë©´ ìƒì„¸ í›„ ì´ì „ í˜ì´ì§€ë¡œ ë³µê·€."""
        if not jobs:
            return jobs
        if not HAS_PLAYWRIGHT:
            print("\nâš ï¸ ìƒì„¸ ìˆ˜ì§‘ì€ Playwright í•„ìš”. pip install playwright && playwright install chromium")
            return jobs
        return self._enrich_jobs_with_details_playwright(jobs, max_details=max_details, list_url=list_url)

    def save_to_csv(self, jobs, filename=None):
        """ê²°ê³¼ë¥¼ csvë¡œ ì €ì¥"""
        if not jobs:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not filename:
            # ì•„ë¬´ëŸ° ê²½ë¡œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì´ python íŒŒì¼ì´ ìˆëŠ” ê³³ì— ì €ì¥ë©ë‹ˆë‹¤.
            filename = f"ì í•_ê³µê³ _{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        df = pd.DataFrame(jobs)
        df.drop(columns=['keyword', 'work_type', 'crawled_at', 'rec_idx', 'title'], errors='ignore', inplace=True)
        # job_role: "ì±„ìš©" ì œê±°
        if 'job_role' in df.columns:
            df['job_role'] = df['job_role'].fillna('').astype(str).str.replace('ì±„ìš©', '', regex=False).str.strip()
        # location: "ì§€ë„ë³´ê¸°", "Â·", "ì£¼ì†Œë³µì‚¬"/"ì£¼ì†Œ ë³µì‚¬" ì œê±°
        if 'location' in df.columns:
            loc = df['location'].fillna('').astype(str)
            for s in ('ì§€ë„ë³´ê¸°', 'Â·', 'ì£¼ì†Œë³µì‚¬', 'ì£¼ì†Œ ë³µì‚¬'):
                loc = loc.str.replace(s, '', regex=False)
            df['location'] = loc.str.replace(r'\s+', ' ', regex=True).str.strip()
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"{len(jobs)}ê°œ ê³µê³ ë¥¼ {filename}ì— ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.")
        return filename
    
    def _get_keyword_stats(self, jobs):
        """ì§ë¬´ë³„ í†µê³„ ìƒì„±"""
        role_counts = {}
        for job in jobs:
            role = job.get('job_role', 'ê¸°íƒ€')
            role_counts[role] = role_counts.get(role, 0) + 1
        stats = [f"{k}({v}ê°œ)" for k, v in role_counts.items()]
        return ", ".join(stats)

    def run_advanced_crawler(self, fetch_details=True, max_details=200, job_roles=None):
        """ì í•: ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ â†’ ê° ê³µê³  ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸í•´ ê¸°ìˆ ìŠ¤íƒÂ·ì£¼ìš”ì—…ë¬´ ë“± ìˆ˜ì§‘"""
        print("ğŸš€ í¬ë¡¤ë§ ì‹œì‘! (ì í•)")

        if job_roles is None:
            job_roles = ['ì „ì²´']
        elif isinstance(job_roles, str):
            job_roles = [job_roles]
        # ì§€ì›í•˜ëŠ” ì§ë¬´ë§Œ í•„í„°
        job_roles = [r for r in job_roles if r in self.jumpit_job_roles]

        # ë¬´í•œ ìŠ¤í¬ë¡¤ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (Playwright) ë˜ëŠ” í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘
        if HAS_PLAYWRIGHT and max_details >= 200:
            unique_jobs = self._fetch_jobs_list_infinite_scroll_playwright(
                target_count=max_details, job_role=job_roles[0], sort='popular'
            )
        else:
            all_jobs = []
            for job_role in job_roles:
                jobs = self.search_jobs_jumpit(sort='popular', max_pages=30, job_role=job_role, target_count=max_details)
                all_jobs.extend(jobs)
            unique_jobs = []
            seen_links = set()
            for job in all_jobs:
                if job.get('link') and job['link'] not in seen_links:
                    unique_jobs.append(job)
                    seen_links.add(job['link'])
            unique_jobs = unique_jobs[:max_details]

        print(f"\nğŸ‰ ë¦¬ìŠ¤íŠ¸ì—ì„œ {len(unique_jobs)}ê°œ ê³ ìœ  ê³µê³  í™•ì¸!")
        print(f"   â†’ ìƒì„¸ ìˆ˜ì§‘ ëŒ€ìƒ: {len(unique_jobs)}ê±´ (ê° ì¹´ë“œ â†’ ìƒì„¸ í˜ì´ì§€ì—ì„œ ê¸°ìˆ ìŠ¤íƒÂ·ì£¼ìš”ì—…ë¬´ ë“± ìˆ˜ì§‘)")

        list_url = None
        if job_roles:
            from urllib.parse import urlencode
            params = {'sort': 'popular'}
            role_param = self.jumpit_job_roles.get(job_roles[0])
            if role_param:
                params['job'] = role_param
            list_url = self.jumpit_positions_url + ('?' + urlencode(params) if params else '')

        if fetch_details and unique_jobs:
            unique_jobs = self.enrich_jobs_with_details(unique_jobs, max_details=max_details, list_url=list_url)

        # CSV ì €ì¥
        if unique_jobs:
            self.save_to_csv(unique_jobs)

        return unique_jobs

if __name__ == "__main__":
    crawler = JumpitCrawler()

    print("\n" + "="*60)
    print("ğŸ¯ ì í•(jumpit.saramin.co.kr) ê°œë°œ ì§ë¬´ë³„ í¬ë¡¤ë§")
    print("="*60)

    # ì§ë¬´: None ë˜ëŠ” ë¹„ë©´ 'ì „ì²´'ë§Œ, ë¦¬ìŠ¤íŠ¸ë¡œ ì§€ì •í•˜ë©´ í•´ë‹¹ ì§ë¬´ë§Œ ìˆ˜ì§‘
    # ì˜ˆ: job_roles=['ì„œë²„/ë°±ì—”ë“œ ê°œë°œì', 'í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì']
    job_roles = None  # ì „ì²´
    # job_roles = ['ì„œë²„/ë°±ì—”ë“œ ê°œë°œì', 'í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì']

    all_jobs = crawler.run_advanced_crawler(
        fetch_details=True,
        max_details=200,
        job_roles=job_roles
    )

    print(f"\nğŸ“Š ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"   - ì´ ê³µê³  ìˆ˜: {len(all_jobs)}")
    if all_jobs:
        print(f"   - ì²« ë²ˆì§¸ ê³µê³ : {all_jobs[0].get('company', '-')} (ì§ë¬´: {all_jobs[0].get('job_role', '-')})")
    print(f"   - CSV ì €ì¥ ì™„ë£Œ")
